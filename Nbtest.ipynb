{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'indent' is an invalid keyword argument for print()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     16\u001b[0m request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39msearch()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m     17\u001b[0m     part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     channelId\u001b[38;5;241m=\u001b[39mCHANNEL_ID,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     maxResults\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Puedes ajustar esto según tus necesidades\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m response \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitems\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'indent' is an invalid keyword argument for print()"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "# Datos de tu proyecto de Google Cloud\n",
    "API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "CHANNEL_ID = 'UC7mJ2EDXFomeDIRFu5FtEbA'\n",
    "\n",
    "# Inicializa el cliente de la API\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# Establecer la fecha desde la cual buscar los videos\n",
    "published_after = datetime(2024, 9, 7).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "# Realiza la consulta a la API para obtener los videos\n",
    "request = youtube.search().list(\n",
    "    part='snippet',\n",
    "    channelId=CHANNEL_ID,\n",
    "    publishedAfter=published_after,\n",
    "    order='date',  # Ordenar por fecha\n",
    "    type='video',  # Asegurarse de que solo busca videos\n",
    "    maxResults=1  # Puedes ajustar esto según tus necesidades\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "print(json.dumps(response['items']),indent = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"kind\": \"youtube#searchListResponse\",\n",
      "    \"etag\": \"tCj1FOheDEdZdaRqmj7KFqBo1i4\",\n",
      "    \"nextPageToken\": \"CAEQAA\",\n",
      "    \"regionCode\": \"AR\",\n",
      "    \"pageInfo\": {\n",
      "        \"totalResults\": 174,\n",
      "        \"resultsPerPage\": 1\n",
      "    },\n",
      "    \"items\": [\n",
      "        {\n",
      "            \"kind\": \"youtube#searchResult\",\n",
      "            \"etag\": \"tj23L4f2lUL4MojvcHwD2J1RUGs\",\n",
      "            \"id\": {\n",
      "                \"kind\": \"youtube#video\",\n",
      "                \"videoId\": \"YaaskpMWLYY\"\n",
      "            },\n",
      "            \"snippet\": {\n",
      "                \"publishedAt\": \"2024-09-21T12:00:36Z\",\n",
      "                \"channelId\": \"UC7mJ2EDXFomeDIRFu5FtEbA\",\n",
      "                \"title\": \"NATI Y BETU CANCELADOS POR LOS GUSTOS DE HELADO\",\n",
      "                \"description\": \"betular #natijota #eialmoldavsky #homeropettinato #olgaenvivo #olga #streaming #stream #shortclip #shorts.\",\n",
      "                \"thumbnails\": {\n",
      "                    \"default\": {\n",
      "                        \"url\": \"https://i.ytimg.com/vi/YaaskpMWLYY/default.jpg\",\n",
      "                        \"width\": 120,\n",
      "                        \"height\": 90\n",
      "                    },\n",
      "                    \"medium\": {\n",
      "                        \"url\": \"https://i.ytimg.com/vi/YaaskpMWLYY/mqdefault.jpg\",\n",
      "                        \"width\": 320,\n",
      "                        \"height\": 180\n",
      "                    },\n",
      "                    \"high\": {\n",
      "                        \"url\": \"https://i.ytimg.com/vi/YaaskpMWLYY/hqdefault.jpg\",\n",
      "                        \"width\": 480,\n",
      "                        \"height\": 360\n",
      "                    }\n",
      "                },\n",
      "                \"channelTitle\": \"OLGA\",\n",
      "                \"liveBroadcastContent\": \"none\",\n",
      "                \"publishTime\": \"2024-09-21T12:00:36Z\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(response,indent = 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "published_after =  datetime(2024, 9, 15).isoformat(\"T\") + \"Z\"\n",
    "request = youtube.search().list(\n",
    "        part='snippet',\n",
    "        channelId='UC7mJ2EDXFomeDIRFu5FtEbA',\n",
    "        publishedAfter=published_after,\n",
    "        order='date',\n",
    "        type='video',\n",
    "    )\n",
    "\n",
    "response = request.execute()\n",
    "response['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                              title  \\\n",
      "0  2024-09-18                       NOLY Y SU ÚLTIMO DÍA DE VIDA   \n",
      "1  2024-09-18  🔴 MALDICION, VA A SER UN DIA HERMOSO con Mario...   \n",
      "2  2024-09-18  VINO LEILA A BANCAR | #QUÉROMPIMOS Completo - ...   \n",
      "3  2024-09-18  CUMPLE ALFRE, DEBATE DE LAS ESTRELLAS DEPORTIV...   \n",
      "4  2024-09-18  #EDICIONESPECIAL | MICAELA&#39;S, MEJORES AMIG...   \n",
      "\n",
      "           published_at  \n",
      "0  2024-09-18T10:00:14Z  \n",
      "1  2024-09-18T10:10:28Z  \n",
      "2  2024-09-18T02:02:09Z  \n",
      "3  2024-09-18T00:45:01Z  \n",
      "4  2024-09-18T03:17:34Z  \n"
     ]
    }
   ],
   "source": [
    "from utils import initialize_youtube_api, get_videos_from_channel, transform_video_data, group_videos_by_date, CHANNEL_IDS\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    "    Tiene tres etapas:\n",
    "    iterar por cada channel y traerme la información sobre qué videos se publicaron\n",
    "    iterar sobre los videos y traernos las statistics y las metemos en un dataframe\n",
    "    subimos el dataframe a redshift\n",
    "    \n",
    "\"\"\"\n",
    "# Inicializamos la API\n",
    "youtube = initialize_youtube_api()\n",
    "\n",
    "# Establezco la fecha de inicio (últimos N días)\n",
    "published_after = datetime(2024, 9, 15).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "all_videos = []\n",
    "\n",
    "# Para cada channel de youtube itero y me traigo los videos\n",
    "for channel_id in CHANNEL_IDS:\n",
    "    videos = get_videos_from_channel(youtube, channel_id, published_after)\n",
    "    transformed_videos = transform_video_data(videos)\n",
    "    all_videos.extend(transformed_videos)\n",
    "\n",
    "# Agrupo por fecha\n",
    "grouped_videos = group_videos_by_date(all_videos)\n",
    "\n",
    "# Convertir los datos agrupados en una lista de diccionarios para DataFrame\n",
    "video_data = []\n",
    "for date, videos in grouped_videos.items():\n",
    "    for video in videos:\n",
    "        video_data.append({\n",
    "            'date': date,\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at']\n",
    "        })\n",
    "\n",
    "# Guardamos los datos en un DataFrame\n",
    "df = pd.DataFrame(video_data)\n",
    "\n",
    "# Guardar en un archivo CSV\n",
    "df.to_csv('youtube_videos.csv', index=False)\n",
    "\n",
    "# Mostrar los primeros registros del DataFrame para verificar\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Inicializamos la API\u001b[39;00m\n\u001b[1;32m      4\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOUTUBE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m youtube \u001b[38;5;241m=\u001b[39m \u001b[43mbuild\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoutube\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv3\u001b[39m\u001b[38;5;124m'\u001b[39m, developerKey\u001b[38;5;241m=\u001b[39mAPI_KEY)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Realizamos la búsqueda del canal por handle (nombre del canal)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39msearch()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m      9\u001b[0m     part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     q\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@VorterixOficial\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     maxResults\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Solo necesitamos el canal\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Inicializamos la API\n",
    "API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# Realizamos la búsqueda del canal por handle (nombre del canal)\n",
    "request = youtube.search().list(\n",
    "    part='snippet',\n",
    "    q='@VorterixOficial',\n",
    "    type='channel',\n",
    "    maxResults=1  # Solo necesitamos el canal\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "# Extraemos el ID del canal de la respuesta\n",
    "for item in response['items']:\n",
    "    channel_id = item['snippet']['channelId']\n",
    "    print(f\"ID del canal: {channel_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from googleapiclient.errors import HttpError\n",
    "import isodate\n",
    "\n",
    "def convert_duration_to_seconds(duration_iso):\n",
    "    # Convierte la duración ISO 8601 a un objeto timedelta\n",
    "    duration = isodate.parse_duration(duration_iso)\n",
    "    return int(duration.total_seconds())\n",
    "\n",
    "def get_channel_info(youtube, channel_id):\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet,statistics',\n",
    "            id=channel_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extraemos la información del canal\n",
    "        channel_info = response['items'][0]\n",
    "        channel_data = {\n",
    "            'channel_name': channel_info['snippet']['title'],\n",
    "            'channel_id': channel_info['id'],\n",
    "            'subscriber_count': channel_info['statistics'].get('subscriberCount', 0)\n",
    "        }\n",
    "\n",
    "        return channel_data\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Error al obtener información del canal {channel_id}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_videos_from_channel(youtube, channel_id, published_after, max_requests=10, sleep_time=1):\n",
    "    videos = []\n",
    "    request_count = 0\n",
    "    \n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            channelId=channel_id,\n",
    "            publishedAfter=published_after,\n",
    "            order='date',\n",
    "            type='video',\n",
    "            maxResults=10  # Puedes ajustar según tus necesidades\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Procesamos la respuesta\n",
    "        for item in response['items']:\n",
    "            video_data = {\n",
    "                'title': item['snippet']['title'],\n",
    "                'published_at': item['snippet']['publishedAt'],\n",
    "                'video_id': item['id']['videoId'],\n",
    "                'video_type': item['snippet'].get('liveBroadcastContent', 'none')  # live, upcoming o none\n",
    "            }\n",
    "            videos.append(video_data)\n",
    "        \n",
    "        request_count += 1\n",
    "        \n",
    "        # Si hemos alcanzado el límite de solicitudes, hacemos un sleep\n",
    "        if request_count % max_requests == 0:\n",
    "            print(f\"Realizadas {request_count} solicitudes. Esperando {sleep_time} segundos para continuar...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Error al obtener videos del canal {channel_id}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado: {e}\")\n",
    "\n",
    "    return videos\n",
    "\n",
    "def get_video_statistics(youtube, video_ids, max_requests=10, sleep_time=1):\n",
    "    video_stats = []\n",
    "    request_count = 0\n",
    "    \n",
    "    try:\n",
    "        stats_request = youtube.videos().list(\n",
    "            part='statistics,contentDetails',\n",
    "            id=','.join(video_ids)  # Pasamos los IDs de los videos como una lista separada por comas\n",
    "        )\n",
    "        stats_response = stats_request.execute()\n",
    "\n",
    "        # Procesamos las estadísticas\n",
    "        for stats in stats_response['items']:\n",
    "            # Convertimos la duración de ISO 8601 a segundos\n",
    "            duration_iso = stats['contentDetails']['duration']\n",
    "            duration_seconds = convert_duration_to_seconds(duration_iso)\n",
    "            \n",
    "            # Verificamos si el video es un YouTube Short (menos de 60 segundos)\n",
    "            is_short = duration_seconds < 60\n",
    "            \n",
    "            stats_data = {\n",
    "                'video_id': stats['id'],\n",
    "                'view_count': stats['statistics'].get('viewCount', 0),\n",
    "                'like_count': stats['statistics'].get('likeCount', 0),\n",
    "                'comment_count': stats['statistics'].get('commentCount', 0),\n",
    "                'duration': duration_seconds,\n",
    "                'is_short': is_short\n",
    "            }\n",
    "            video_stats.append(stats_data)\n",
    "        \n",
    "        request_count += 1\n",
    "\n",
    "        # Si hemos alcanzado el límite de solicitudes, hacemos un sleep\n",
    "        if request_count % max_requests == 0:\n",
    "            print(f\"Realizadas {request_count} solicitudes. Esperando {sleep_time} segundos para continuar...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Error al obtener estadísticas de videos: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado: {e}\")\n",
    "\n",
    "    return video_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtenemos la información del canal\n",
    "channel_info = get_channel_info(youtube, channel_id)\n",
    "\n",
    "\n",
    "# Obtener la fecha de consulta actual\n",
    "consulta_fecha = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Crear un DataFrame con la información de suscriptores del canal\n",
    "df_subscribers = pd.DataFrame([{\n",
    "    'channel_name': channel_info['channel_name'],\n",
    "    'channel_id': channel_info['channel_id'],\n",
    "    'consulta_fecha': consulta_fecha,\n",
    "    'subscriber_count': channel_info['subscriber_count']\n",
    "}])\n",
    "\n",
    "# Verificamos que se haya obtenido la información del canal\n",
    "if channel_info:\n",
    "    # Creamos una lista vacía para almacenar los datos combinados\n",
    "    combined_data = []\n",
    "\n",
    "    # Iteramos sobre los videos y sus estadísticas\n",
    "    for video, stats in zip(videos, video_stats):\n",
    "        # Combinamos la información de los videos, las estadísticas y el canal en un solo diccionario\n",
    "        combined_data.append({\n",
    "            'channel_name': channel_info['channel_name'],\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at'],\n",
    "            'video_id': video['video_id'],\n",
    "            'video_type': video['video_type'],\n",
    "            'view_count': stats['view_count'],\n",
    "            'like_count': stats['like_count'],\n",
    "            'comment_count': stats['comment_count'],\n",
    "            'duration_seconds': stats['duration'],\n",
    "            'is_short': stats['is_short']\n",
    "        })\n",
    "\n",
    "    # Convertimos la lista de diccionarios en un DataFrame de pandas\n",
    "    df_videos = pd.DataFrame(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_type</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>is_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>MIRANDA Y UN TEMAZO DE CRISTIAN CASTRO</td>\n",
       "      <td>2024-09-21T14:00:35Z</td>\n",
       "      <td>XjEX99nbf6Q</td>\n",
       "      <td>none</td>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>NATI Y BETU CANCELADOS POR LOS GUSTOS DE HELADO</td>\n",
       "      <td>2024-09-21T12:00:36Z</td>\n",
       "      <td>YaaskpMWLYY</td>\n",
       "      <td>none</td>\n",
       "      <td>573</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>GREGO DE CHETO A HUMILDE POR UNA TORTA FRITA</td>\n",
       "      <td>2024-09-21T10:00:15Z</td>\n",
       "      <td>HzU45a7Pxjc</td>\n",
       "      <td>none</td>\n",
       "      <td>1135</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>OLIVIA FIRPO CUMPLIÓ SU SUEÑO EN EL TEATRO COLÓN</td>\n",
       "      <td>2024-09-21T00:19:15Z</td>\n",
       "      <td>F7ZejuxyrDw</td>\n",
       "      <td>none</td>\n",
       "      <td>4344</td>\n",
       "      <td>273</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>CARO Y SOFI GONET DEL ODIO AL AMOR POR LA MODA</td>\n",
       "      <td>2024-09-20T20:00:17Z</td>\n",
       "      <td>ma15lzAoKx4</td>\n",
       "      <td>none</td>\n",
       "      <td>1498</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>AGUANTE OEISIS | Soñé que Volaba | COMPLETO 20/9</td>\n",
       "      <td>2024-09-20T18:07:37Z</td>\n",
       "      <td>OZeLwvFPfGk</td>\n",
       "      <td>none</td>\n",
       "      <td>56980</td>\n",
       "      <td>1649</td>\n",
       "      <td>182</td>\n",
       "      <td>8283</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>RECHI Y EL MEJOR CHISTE DE LA HISTORIA</td>\n",
       "      <td>2024-09-20T18:00:26Z</td>\n",
       "      <td>i3YkcpDXcP4</td>\n",
       "      <td>none</td>\n",
       "      <td>11893</td>\n",
       "      <td>769</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>ESPECIAL SUPERCLÁSICO: BOCA - RIVER en OLGA co...</td>\n",
       "      <td>2024-09-20T17:52:47Z</td>\n",
       "      <td>XXqVT9gaw_E</td>\n",
       "      <td>upcoming</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>GIME SE PELEÓ CON TODOS</td>\n",
       "      <td>2024-09-20T16:00:45Z</td>\n",
       "      <td>gFhK3npHMRo</td>\n",
       "      <td>none</td>\n",
       "      <td>2244</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>INTELIGENCIA ARTIFICIAL y la PREVIA del SUPERC...</td>\n",
       "      <td>2024-09-20T15:24:24Z</td>\n",
       "      <td>5xcXpVf3IPU</td>\n",
       "      <td>none</td>\n",
       "      <td>86932</td>\n",
       "      <td>2380</td>\n",
       "      <td>149</td>\n",
       "      <td>7261</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel_name                channel_id  \\\n",
       "0         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "1         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "2         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "3         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "4         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "5         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "6         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "7         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "8         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "9         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "\n",
       "                                               title          published_at  \\\n",
       "0             MIRANDA Y UN TEMAZO DE CRISTIAN CASTRO  2024-09-21T14:00:35Z   \n",
       "1    NATI Y BETU CANCELADOS POR LOS GUSTOS DE HELADO  2024-09-21T12:00:36Z   \n",
       "2       GREGO DE CHETO A HUMILDE POR UNA TORTA FRITA  2024-09-21T10:00:15Z   \n",
       "3   OLIVIA FIRPO CUMPLIÓ SU SUEÑO EN EL TEATRO COLÓN  2024-09-21T00:19:15Z   \n",
       "4     CARO Y SOFI GONET DEL ODIO AL AMOR POR LA MODA  2024-09-20T20:00:17Z   \n",
       "5   AGUANTE OEISIS | Soñé que Volaba | COMPLETO 20/9  2024-09-20T18:07:37Z   \n",
       "6             RECHI Y EL MEJOR CHISTE DE LA HISTORIA  2024-09-20T18:00:26Z   \n",
       "7  ESPECIAL SUPERCLÁSICO: BOCA - RIVER en OLGA co...  2024-09-20T17:52:47Z   \n",
       "8                            GIME SE PELEÓ CON TODOS  2024-09-20T16:00:45Z   \n",
       "9  INTELIGENCIA ARTIFICIAL y la PREVIA del SUPERC...  2024-09-20T15:24:24Z   \n",
       "\n",
       "      video_id video_type view_count like_count comment_count  \\\n",
       "0  XjEX99nbf6Q       none         35         15             0   \n",
       "1  YaaskpMWLYY       none        573        127             1   \n",
       "2  HzU45a7Pxjc       none       1135         83             0   \n",
       "3  F7ZejuxyrDw       none       4344        273             2   \n",
       "4  ma15lzAoKx4       none       1498        103             0   \n",
       "5  OZeLwvFPfGk       none      56980       1649           182   \n",
       "6  i3YkcpDXcP4       none      11893        769             6   \n",
       "7  XXqVT9gaw_E   upcoming          0          8             0   \n",
       "8  gFhK3npHMRo       none       2244        202             0   \n",
       "9  5xcXpVf3IPU       none      86932       2380           149   \n",
       "\n",
       "   duration_seconds  is_short  \n",
       "0                36      True  \n",
       "1                60     False  \n",
       "2                57      True  \n",
       "3                55      True  \n",
       "4                60     False  \n",
       "5              8283     False  \n",
       "6                60     False  \n",
       "7                 0      True  \n",
       "8                59      True  \n",
       "9              7261     False  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos['likes_per_view'] = df_videos['like_count'] / df_videos['view_count']\n",
    "df_videos['comments_per_view'] = df_videos['comment_count'] / df_videos['view_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import initialize_youtube_api, get_videos_from_channel, get_video_statistics, get_channel_info, group_videos_by_date, CHANNEL_IDS\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "    Tiene tres etapas:\n",
    "    1. Iterar por cada channel y traer la información sobre qué videos se publicaron\n",
    "    2. Iterar sobre los videos y traernos las estadísticas y las metemos en un DataFrame\n",
    "    3. Subir los DataFrames (videos y suscriptores) a Redshift o guardarlos en CSV\n",
    "    \n",
    "    WARNING: Este código es para correr de manera diaria.\n",
    "    Una vulnerabilidad que tiene es que la tabla con cantidad de suscriptores si se corre desde una fecha en particular, no va a obtener los subs desde esa fecha, si no de manera diaria\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Inicializamos la API\n",
    "youtube = initialize_youtube_api()\n",
    "\n",
    "# Establezco la fecha de inicio (últimos N días)\n",
    "published_after = (datetime.now() - timedelta(days=7)).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "all_videos = []\n",
    "all_video_stats = []\n",
    "df_subscribers_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Se itera para cada channel de YouTube y sacamos estadísticas: cantindad de subs a la fecha de consulta, sus videos y para cada video métricas de views, comentarios y likes\n",
    "for channel_id in CHANNEL_IDS:\n",
    "    # Obtenemos la información del canal (nombre, id, suscriptores)\n",
    "    channel_info = get_channel_info(youtube, channel_id)\n",
    "    \n",
    "    # Obtener la fecha de consulta actual\n",
    "    consulta_fecha = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Crear un df con la info de suscriptores del canal\n",
    "    df_subscribers_list.append({\n",
    "        'channel_name': channel_info['channel_name'],\n",
    "        'channel_id': channel_info['channel_id'],\n",
    "        'consulta_fecha': consulta_fecha,\n",
    "        'subscriber_count': channel_info['subscriber_count']\n",
    "    })\n",
    "\n",
    "    videos = get_videos_from_channel(youtube, channel_id, published_after)\n",
    "    \n",
    "    # Filtramos videos que contengan alguno de los campos que necesitamos para obtener el video ID. \n",
    "    # Hacemos esto por si cambia el nombre de la key con la que se guarda el video_id. \n",
    "    video_ids = []\n",
    "    for video in videos:\n",
    "        if 'video_id' in video:\n",
    "            video_ids.append(video['video_id'])\n",
    "        elif 'id' in video and isinstance(video['id'], dict) and 'videoId' in video['id']:\n",
    "            video_ids.append(video['id']['videoId'])\n",
    "        else:\n",
    "            # Si no se encuentra el video_id, imprimimos el video para depuración\n",
    "            print(f\"Video sin 'video_id' o 'id' válido detectado: {video}\")\n",
    "    \n",
    "    # Las stats de los videos\n",
    "    video_stats = get_video_statistics(youtube, video_ids)\n",
    "    \n",
    "    # Agregamos los videos y las estadísticas a una lista combinada\n",
    "    for video, stats in zip(videos, video_stats):\n",
    "        video_id = video.get('video_id') or (video['id']['videoId'] if isinstance(video['id'], dict) and 'videoId' in video['id'] else 'N/A')\n",
    "        \n",
    "        all_videos.append({\n",
    "            'channel_name': channel_info['channel_name'],\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at'],\n",
    "            'video_id': video['video_id'],\n",
    "            'video_type': video['video_type'],\n",
    "            'view_count': stats['view_count'],\n",
    "            'like_count': stats['like_count'],\n",
    "            'comment_count': stats['comment_count'],\n",
    "            'duration_seconds': stats['duration'],\n",
    "            'is_short': stats['is_short']\n",
    "        })\n",
    "\n",
    "# Pasamos a un df los videos\n",
    "df_videos = pd.DataFrame(all_videos)\n",
    "\n",
    "# Hago una transformación de los datos \n",
    "df_videos['like_count'] = pd.to_numeric(df_videos['like_count'], errors='coerce')\n",
    "df_videos['view_count'] = pd.to_numeric(df_videos['view_count'], errors='coerce')\n",
    "df_videos['comment_count'] = pd.to_numeric(df_videos['comment_count'], errors='coerce')\n",
    "\n",
    "\n",
    "df_videos['likes_per_view'] = df_videos['like_count'] / df_videos['view_count']\n",
    "df_videos['comments_per_view'] = df_videos['comment_count'] / df_videos['view_count']\n",
    "\n",
    "# Pasamos a un df los logs de suscriptores\n",
    "df_subscribers = pd.DataFrame(df_subscribers_list)\n",
    "\n",
    "\n",
    "\n",
    "# Guardar los DataFrames en archivos CSV\n",
    "df_videos.to_csv('youtube_videos.csv', index=False)\n",
    "df_subscribers.to_csv('youtube_subscribers_history.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7_/ydy5jwh51735b2j1dqgskh7c0000gn/T/ipykernel_6267/2684085640.py:14: SADeprecationWarning: The dbapi() classmethod on dialect classes has been renamed to import_dbapi().  Implement an import_dbapi() classmethod directly on class <class 'sqlalchemy_redshift.dialect.RedshiftDialect_psycopg2'> to remove this warning; the old .dbapi() classmethod may be maintained for backwards compatibility.\n",
      "  engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Credenciales de conexión a Redshift\n",
    "DATABASE_TYPE = 'redshift+psycopg2'\n",
    "DBAPI = 'psycopg2'\n",
    "ENDPOINT = 'redshift-pda-cluster.cnuimntownzt.us-east-2.redshift.amazonaws.com'\n",
    "USER = '2024_franco_santoliquido'\n",
    "PASSWORD = 'L6^&9!2$xQ'\n",
    "PORT = 5439\n",
    "DATABASE = 'pda'\n",
    "\n",
    "# Cadena de conexión a Redshift\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos.to_sql('2024_franco_santoliquido_schema.youtube_videos_stg', engine, index=False, if_exists='replace')\n",
    "df_subscribers.to_sql('2024_franco_santoliquido_schema.youtube_subscribers_stg', engine, index=False, if_exists='replace') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chili Parker: Hay que autoinmolarse contra las ideas de la izquierda por el futuro de nuestros hijos',\n",
       "  'published_at': '2024-09-22T16:27:42Z',\n",
       "  'video_id': 'g1W6OEL2bW4',\n",
       "  'video_type': 'none'},\n",
       " {'title': '🚨 CHILI PARKER EN LA MISA 🚨 LA MORCILLA CON BÁCULO y CHADMANZIO | La Misa de Dan',\n",
       "  'published_at': '2024-09-21T02:26:15Z',\n",
       "  'video_id': 'KGkJcg5FULQ',\n",
       "  'video_type': 'none'},\n",
       " {'title': 'ZURDO QUEDA EN RIDÍCULO #lamisa #podcast #milei',\n",
       "  'published_at': '2024-09-20T22:45:16Z',\n",
       "  'video_id': 'KQf8hZj3P3U',\n",
       "  'video_type': 'none'},\n",
       " {'title': 'ZURDOS UNIVERSITARIOS QUEDAN EN RIDÍCULO',\n",
       "  'published_at': '2024-09-20T21:00:07Z',\n",
       "  'video_id': 'cUHm5TYjfLg',\n",
       "  'video_type': 'none'},\n",
       " {'title': '¿MEJOR PRESIDENTE? LO QUE NADIE TE CONTÓ DE JULIO.A.ROCA',\n",
       "  'published_at': '2024-09-20T19:00:06Z',\n",
       "  'video_id': 'bV-w9Z1SS4g',\n",
       "  'video_type': 'none'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Obtenemos la información del canal\n",
    "channel_info = get_channel_info(youtube, channel_id)\n",
    "\n",
    "# Verificamos que se haya obtenido la información del canal\n",
    "if channel_info:\n",
    "    # Creamos una lista vacía para almacenar los datos combinados\n",
    "    combined_data = []\n",
    "\n",
    "    # Iteramos sobre los videos y sus estadísticas\n",
    "    for video, stats in zip(videos, video_stats):\n",
    "        # Combinamos la información de los videos, las estadísticas y el canal en un solo diccionario\n",
    "        combined_data.append({\n",
    "            'channel_name': channel_info['channel_name'],\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at'],\n",
    "            'video_id': video['video_id'],\n",
    "            'video_type': video['video_type'],\n",
    "            'view_count': stats['view_count'],\n",
    "            'like_count': stats['like_count'],\n",
    "            'comment_count': stats['comment_count'],\n",
    "            'duration_seconds': stats['duration'],\n",
    "            'is_short': stats['is_short']\n",
    "        })\n",
    "\n",
    "    # Convertimos la lista de diccionarios en un DataFrame de pandas\n",
    "    df_videos = pd.DataFrame(combined_data)\n",
    "\n",
    "\n",
    "df['likes_per_view'] = df['like_count']/df['view_count']\n",
    "df['comments_per_view'] = df['comment_count'] / df['view_count']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.errors import HttpError\n",
    "import isodate\n",
    "\n",
    "\n",
    "# Lista de canales a monitorear\n",
    "CHANNEL_IDS = [\n",
    "    'UC7mJ2EDXFomeDIRFu5FtEbA', \n",
    "    'UCvCTWHCbBC0b9UIeLeNs8ug',\n",
    "    'UCWSfXECGo1qK_H7SXRaUSMg',\n",
    "    'UCTHaNTsP7hsVgBxARZTuajw',\n",
    "    'UC4mdhKZXjrKoq5aVG6juHEg' \n",
    "]\n",
    "\n",
    "\n",
    "#Tres funciones para subir a redshift: una conecta, la otra uploadea en raw y la otra hace el upsert\n",
    "def connect_to_redshift():\n",
    "\n",
    "    DATABASE_TYPE = 'redshift+psycopg2'\n",
    "    DBAPI = 'psycopg2'\n",
    "    ENDPOINT = os.getenv('REDSHIFT_ENDPOINT')\n",
    "    USER = os.getenv('REDSHIFT_USER')        \n",
    "    PASSWORD = os.getenv('REDSHIFT_PASSWORD')\n",
    "    PORT = 5439\n",
    "    DATABASE = os.getenv('REDSHIFT_DATABASE')\n",
    "    engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "    return engine\n",
    "\n",
    "def upload_to_redshift(engine, df, destintation_table, schema):\n",
    "    df.to_sql(destintation_table, engine, schema, index=False, if_exists='replace')\n",
    "\n",
    "def run_sql_queries(engine):\n",
    "    # Obtener la ruta absoluta del archivo queries.sql\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))  # Obtiene la ruta absoluta de module_etl\n",
    "    queries_file_path = os.path.join(base_dir, 'queries.sql')  # Se une para llegar a /queries.sql\n",
    "    \n",
    "    if not os.path.exists(queries_file_path):\n",
    "        raise FileNotFoundError(f\"No se encontró el archivo queries.sql en la ruta: {queries_file_path}\")\n",
    "    \n",
    "    with open(queries_file_path, 'r') as file:\n",
    "        sql_queries = file.read()\n",
    "    queries = sql_queries.split(';')\n",
    "    \n",
    "    # Ejecuto las queries del archivo queries que tiene el upsert para hacer\n",
    "    with engine.connect() as connection:\n",
    "        for query in queries:\n",
    "            query = query.strip()\n",
    "            if query:  # Si la consulta no está vacía\n",
    "                connection.execute(query)\n",
    "                print(f\"Ejecutada la consulta:\\n{query}\\n\")\n",
    "\n",
    "\n",
    "# Convierto duracion que da Youtube ISO 8601 a un objeto de tiempo\n",
    "def convert_duration_to_seconds(duration_iso):\n",
    "\n",
    "    duration = isodate.parse_duration(duration_iso)\n",
    "    return int(duration.total_seconds())\n",
    "\n",
    "# Inicio la api de Youtube con mis credenciales\n",
    "def initialize_youtube_api():\n",
    "    API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "    return youtube\n",
    "\n",
    "\n",
    "# Funcion para buscar videos de un canal, con retrys\n",
    "\n",
    "def get_videos_from_channel(youtube, channel_id, published_after, max_requests=10, sleep_time=1, max_retries=3):\n",
    "    videos = []\n",
    "    request_count = 0\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            request = youtube.search().list(\n",
    "                part='snippet',\n",
    "                channelId=channel_id,\n",
    "                publishedAfter=published_after,\n",
    "                order='date',\n",
    "                type='video'\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # De la respuesta sacamos titulo, momento de publicacion, id del video y tipo de video (live, upcoming o none)\n",
    "            for item in response['items']:\n",
    "                video_data = {\n",
    "                    'title': item['snippet']['title'],\n",
    "                    'published_at': item['snippet']['publishedAt'],\n",
    "                    'video_id': item['id']['videoId'],\n",
    "                    'video_type': item['snippet'].get('liveBroadcastContent', 'none')  # live, upcoming o none\n",
    "                }\n",
    "                videos.append(video_data)\n",
    "            \n",
    "            request_count += 1\n",
    "            \n",
    "            # Si se alcanza el límite de requests, metemos un sleep comentado\n",
    "            if request_count % max_requests == 0:\n",
    "                print(f\"Realizadas {request_count} solicitudes. Esperando {sleep_time} segundos para continuar...\")\n",
    "                time.sleep(sleep_time)\n",
    "            \n",
    "            return videos\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"Error al obtener videos del canal {channel_id}: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurrió un error inesperado: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        \n",
    "        # Meto otro sleep para retry\n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "    raise Exception(f\"No se pudo obtener los videos del canal {channel_id} después de {max_retries} intentos.\")\n",
    "\n",
    "#Busca la información del canal de Youtube que le pases\n",
    "def get_channel_info(youtube, channel_id, max_retries=3, sleep_time=1):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            request = youtube.channels().list(\n",
    "                part='snippet,statistics',\n",
    "                id=channel_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # la respuesta tiene una key \"items\" contiene la data del canal y la cantidad de suscriptores\n",
    "            channel_info = response['items'][0]\n",
    "            channel_data = {\n",
    "                'channel_name': channel_info['snippet']['title'],\n",
    "                'channel_id': channel_info['id'],\n",
    "                'subscriber_count': channel_info['statistics'].get('subscriberCount', 0)\n",
    "            }\n",
    "\n",
    "            return channel_data\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"Error al obtener información del canal {channel_id}: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurrió un error inesperado: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        \n",
    "        # Esperamos antes de intentar nuevamente\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    # Tiro el error para que pinche si no puedo obtener la información del canal\n",
    "    raise Exception(f\"No se pudo obtener la información del canal {channel_id} después de {max_retries} intentos.\")\n",
    "\n",
    "def get_video_statistics(youtube, video_ids, max_requests=10, sleep_time=2, max_retries=3):\n",
    "    video_stats = []\n",
    "    request_count = 0\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Pasamos los IDs de los videos como una lista\n",
    "            stats_request = youtube.videos().list(\n",
    "                part='statistics,contentDetails',\n",
    "                id=','.join(video_ids)\n",
    "            )\n",
    "            stats_response = stats_request.execute()\n",
    "\n",
    "            # Procesamos las estadísticas\n",
    "            for stats in stats_response['items']:\n",
    "                duration_iso = stats['contentDetails'].get('duration', None)  # Ajuste aquí\n",
    "                if duration_iso is None:\n",
    "                    print(f\"El video {stats['id']} no tiene duración, omitimos el dato.\")\n",
    "                    continue \n",
    "\n",
    "                duration_seconds = convert_duration_to_seconds(duration_iso)\n",
    "                \n",
    "                # Verificamos si el video es un YouTube Short (menos de 60 segundos)\n",
    "                is_short = duration_seconds < 60\n",
    "                \n",
    "                stats_data = {\n",
    "                    'video_id': stats['id'],\n",
    "                    'view_count': stats['statistics'].get('viewCount', 0),\n",
    "                    'like_count': stats['statistics'].get('likeCount', 0),\n",
    "                    'comment_count': stats['statistics'].get('commentCount', 0),\n",
    "                    'duration': duration_seconds,\n",
    "                    'is_short': is_short\n",
    "                }\n",
    "                video_stats.append(stats_data)\n",
    "            \n",
    "            request_count += 1\n",
    "\n",
    "            # Si se llega el límite de solicitudes, hacemos un sleep\n",
    "            if request_count % max_requests == 0:\n",
    "                print(f\"Realizadas {request_count} solicitudes. Esperando {sleep_time} segundos para continuar...\")\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "            return video_stats\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"Error al obtener estadísticas de videos: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Ocurrió un error de clave: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurrió un error inesperado: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        \n",
    "        # Esperamos para intentar nuevamente\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    # Tiro el error para que pinche si no puedo obtener las estadísticas del canal\n",
    "    raise Exception(f\"No se pudieron obtener las estadísticas de videos después de {max_retries} intentos.\")\n",
    "\n",
    "\n",
    "\n",
    "# Función para sacar del json el datos que necesito\n",
    "def transform_video_data(videos):\n",
    "    transformed_data = []\n",
    "    for video in videos:\n",
    "        title = video['snippet']['title']\n",
    "        published_at = video['snippet']['publishedAt']\n",
    "        transformed_data.append({\n",
    "            'title': title,\n",
    "            'published_at': published_at\n",
    "        })\n",
    "    return transformed_data\n",
    "\n",
    "# Función para agrupar datos por dia \n",
    "def group_videos_by_date(videos):\n",
    "    grouped_data = {}\n",
    "    for video in videos:\n",
    "        date = video['published_at'].split(\"T\")[0]\n",
    "        if date not in grouped_data:\n",
    "            grouped_data[date] = []\n",
    "        grouped_data[date].append(video)\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "    Tiene tres etapas:\n",
    "    1. Iterar por cada channel y traer la información sobre qué videos se publicaron\n",
    "    2. Iterar sobre los videos y traernos las estadísticas y las metemos en un DataFrame\n",
    "    3. Subir los DataFrames (videos y suscriptores) a Redshift o guardarlos en CSV\n",
    "    \n",
    "    WARNING: Este código es para correr de manera diaria.\n",
    "    Una vulnerabilidad que tiene es que la tabla con cantidad de suscriptores si se corre desde una fecha en particular, no va a obtener los subs desde esa fecha, si no de manera diaria\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Inicializamos la API\n",
    "youtube = initialize_youtube_api()\n",
    "\n",
    "# Establezco la fecha de inicio (últimos N días)\n",
    "published_after = (datetime.now() - timedelta(days=7)).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "all_videos = []\n",
    "all_video_stats = []\n",
    "df_subscribers_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Se itera para cada channel de YouTube y sacamos estadísticas: cantindad de subs a la fecha de consulta, sus videos y para cada video métricas de views, comentarios y likes\n",
    "for channel_id in CHANNEL_IDS:\n",
    "    # Obtenemos la información del canal (nombre, id, suscriptores)\n",
    "    channel_info = get_channel_info(youtube, channel_id)\n",
    "    \n",
    "    # Obtener la fecha de consulta actual\n",
    "    consulta_fecha = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Crear un df con la info de suscriptores del canal\n",
    "    df_subscribers_list.append({\n",
    "        'channel_name': channel_info['channel_name'],\n",
    "        'channel_id': channel_info['channel_id'],\n",
    "        'consulta_fecha': consulta_fecha,\n",
    "        'subscriber_count': channel_info['subscriber_count']\n",
    "    })\n",
    "\n",
    "    videos = get_videos_from_channel(youtube, channel_id, published_after)\n",
    "    \n",
    "    # Filtramos videos que contengan alguno de los campos que necesitamos para obtener el video ID. \n",
    "    # Hacemos esto por si cambia el nombre de la key con la que se guarda el video_id. \n",
    "    video_ids = []\n",
    "    for video in videos:\n",
    "        if 'video_id' in video:\n",
    "            video_ids.append(video['video_id'])\n",
    "        elif 'id' in video and isinstance(video['id'], dict) and 'videoId' in video['id']:\n",
    "            video_ids.append(video['id']['videoId'])\n",
    "        else:\n",
    "            # Si no se encuentra el video_id, imprimimos el video para depuración\n",
    "            print(f\"Video sin 'video_id' o 'id' válido detectado: {video}\")\n",
    "    \n",
    "    # Las stats de los videos\n",
    "    video_stats = get_video_statistics(youtube, video_ids)\n",
    "    \n",
    "    # Agregamos los videos y las estadísticas a una lista combinada\n",
    "    for video, stats in zip(videos, video_stats):\n",
    "        video_id = video.get('video_id') or (video['id']['videoId'] if isinstance(video['id'], dict) and 'videoId' in video['id'] else 'N/A')\n",
    "        \n",
    "        all_videos.append({\n",
    "            'channel_name': channel_info['channel_name'],\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at'],\n",
    "            'video_id': video['video_id'],\n",
    "            'video_type': video['video_type'],\n",
    "            'view_count': stats['view_count'],\n",
    "            'like_count': stats['like_count'],\n",
    "            'comment_count': stats['comment_count'],\n",
    "            'duration_seconds': stats['duration'],\n",
    "            'is_short': stats['is_short']\n",
    "        })\n",
    "\n",
    "# Pasamos a un df los videos\n",
    "df_videos = pd.DataFrame(all_videos)\n",
    "\n",
    "# Hago una transformación de los datos \n",
    "df_videos['like_count'] = pd.to_numeric(df_videos['like_count'], errors='coerce')\n",
    "df_videos['view_count'] = pd.to_numeric(df_videos['view_count'], errors='coerce')\n",
    "df_videos['comment_count'] = pd.to_numeric(df_videos['comment_count'], errors='coerce')\n",
    "\n",
    "\n",
    "df_videos['likes_per_view'] = df_videos['like_count'] / df_videos['view_count']\n",
    "df_videos['comments_per_view'] = df_videos['comment_count'] / df_videos['view_count']\n",
    "\n",
    "# Pasamos a un df los logs de suscriptores\n",
    "df_subscribers = pd.DataFrame(df_subscribers_list)\n",
    "\n",
    "#Iniciamos el engine\n",
    "#engine_rs = connect_to_redshift()\n",
    "\n",
    "#Inserto la raw data\n",
    "#upload_to_redshift(engine_rs, df_videos, 'pda.2024_franco_santoliquido_schema.youtube_videos_stg')\n",
    "#upload_to_redshift(engine_rs, df_subscribers, 'pda.2024_franco_santoliquido_schema.youtube_subscribers_stg')\n",
    "\n",
    "#Corro lo que está en el file queries.sql que tiene el upsert a la tabla final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_redshift():\n",
    "    DATABASE_TYPE = 'redshift+psycopg2'\n",
    "    DBAPI = 'psycopg2'\n",
    "    ENDPOINT = 'redshift-pda-cluster.cnuimntownzt.us-east-2.redshift.amazonaws.com'\n",
    "    USER = '2024_franco_santoliquido'\n",
    "    PASSWORD = 'L6^&9!2$xQ'\n",
    "    PORT = 5439\n",
    "    DATABASE = 'pda'\n",
    "\n",
    "    engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "    return engine\n",
    "def upload_to_redshift(engine, df, destintation_table, schema):\n",
    "    df.to_sql(destintation_table, engine, schema, index=False, if_exists='replace')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejecutar las queries directamente usando SQLAlchemy\n",
    "queries = \"\"\"\n",
    "-- Update e insert para la tabla de videos\n",
    "UPDATE pda.\"2024_franco_santoliquido_schema\".BT_YOUTUBE_VIDEO_STATS DW\n",
    "SET \n",
    "    YT_VIEW_COUNT_7D = STG.view_count,\n",
    "    YT_LIKE_COUNT_7D = STG.like_count,\n",
    "    YT_COMMENT_COUNT_7D = STG.comment_count,\n",
    "    AUD_UPD_ID = CURRENT_DATE,\n",
    "    AUD_UPDATED_FROM = 'ETL_YOUTUBE_CDE_PDA'\n",
    "FROM \n",
    "    pda.\"2024_franco_santoliquido_schema\".youtube_videos_stg STG\n",
    "WHERE  \n",
    "    STG.video_id = DW.YT_VIDEO_ID\n",
    "    AND STG.published_at >= DATEADD(day, -7, CURRENT_DATE)\n",
    "    ;\n",
    "\n",
    "INSERT INTO pda.\"2024_franco_santoliquido_schema\".BT_YOUTUBE_VIDEO_STATS (\n",
    "    YT_CHANNEL_NAME,\n",
    "    YT_CHANNEL_ID,\n",
    "    YT_TITLE_NAME,\n",
    "    YT_DATE_PUBLISHED,\n",
    "    YT_VIDEO_ID,\n",
    "    YT_VIDEO_TYPE,\n",
    "    YT_VIEW_COUNT_7D,\n",
    "    YT_LIKE_COUNT_7D,\n",
    "    YT_COMMENT_COUNT_7D,\n",
    "    YT_DURATION_SECS,\n",
    "    YT_IS_SHORT_FLAG,\n",
    "    AUD_UPD_ID,\n",
    "    AUD_INS_DATE,\n",
    "    AUD_UPDATED_FROM\n",
    ")\n",
    "SELECT \n",
    "    STG.channel_name,\n",
    "    STG.channel_id,\n",
    "    STG.title,\n",
    "    STG.published_at,\n",
    "    STG.video_id,\n",
    "    STG.video_type,\n",
    "    STG.view_count,\n",
    "    STG.like_count,\n",
    "    STG.comment_count,\n",
    "    STG.duration_seconds,\n",
    "    STG.is_short,\n",
    "    CURRENT_DATE,\n",
    "    CURRENT_DATE, \n",
    "    'ETL_YOUTUBE_CDE_PDA'\n",
    "FROM \n",
    "    pda.\"2024_franco_santoliquido_schema\".youtube_videos_stg STG\n",
    "LEFT JOIN \n",
    "    pda.\"2024_franco_santoliquido_schema\".BT_YOUTUBE_VIDEO_STATS DW\n",
    "    ON STG.video_id = DW.YT_VIDEO_ID\n",
    "WHERE \n",
    "    DW.YT_VIDEO_ID IS NULL;\n",
    "\n",
    "-- Insert para la tabla de suscriptores\n",
    "\n",
    "INSERT INTO pda.\"2024_franco_santoliquido_schema\".LG_CHANNEL_SUBSCRIBERS (\n",
    "    YT_CHANNEL_NAME,\n",
    "    YT_CHANNEL_ID,\n",
    "    YT_DATE_ID,\n",
    "    YT_SUSCRIBER_COUNT,\n",
    "    AUD_INS_DATE,\n",
    "    AUD_UPDATED_FROM\n",
    ")\n",
    "SELECT \n",
    "    STG.channel_name,\n",
    "    STG.channel_id,\n",
    "    STG.consulta_fecha,\n",
    "    STG.subscriber_count,\n",
    "    CURRENT_DATE,\n",
    "    'ETL_YOUTUBE_CDE_PDA'\n",
    "FROM \n",
    "    pda.\"2024_franco_santoliquido_schema\".youtube_subscribers_stg STG\n",
    "LEFT JOIN \n",
    "    pda.\"2024_franco_santoliquido_schema\".LG_CHANNEL_SUBSCRIBERS DW\n",
    "ON \n",
    "    STG.channel_id = DW.YT_CHANNEL_ID\n",
    "    AND STG.consulta_fecha = DW.YT_DATE_ID\n",
    "WHERE \n",
    "    DW.YT_CHANNEL_ID IS NULL\n",
    "    ;\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que ya tienes tus DataFrames creados (df_videos y df_subscribers):\n",
    "# Sube los DataFrames con el nombre del esquema ajustado\n",
    "\n",
    "upload_to_redshift(engine, df_videos, 'youtube_videos_stg', schema='2024_franco_santoliquido_schema' )\n",
    "upload_to_redshift(engine, df_videos, 'youtube_subscribers_stg', schema='2024_franco_santoliquido_schema' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_type</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>is_short</th>\n",
       "      <th>likes_per_view</th>\n",
       "      <th>comments_per_view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>BETU ES EL NUEVO REY DE LOS MACARONS</td>\n",
       "      <td>2024-10-05T22:00:33Z</td>\n",
       "      <td>gtTM9GJ8nZU</td>\n",
       "      <td>none</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>True</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>EL PELAO FILOSOFÓ SOBRE EL MATE</td>\n",
       "      <td>2024-10-05T20:00:08Z</td>\n",
       "      <td>OppCmaXtZME</td>\n",
       "      <td>none</td>\n",
       "      <td>197</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>True</td>\n",
       "      <td>0.274112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>HOMI ELEVANDO EL NIVEL DEL PROGRAMA</td>\n",
       "      <td>2024-10-05T18:00:11Z</td>\n",
       "      <td>Q3LCtQoVFfE</td>\n",
       "      <td>none</td>\n",
       "      <td>3214</td>\n",
       "      <td>327</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>True</td>\n",
       "      <td>0.101742</td>\n",
       "      <td>0.000622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>EL MITO DEL PETISO PIJUDO EN HABLEMOS SIN SABER</td>\n",
       "      <td>2024-10-05T16:00:16Z</td>\n",
       "      <td>J8KDqUj9B2g</td>\n",
       "      <td>none</td>\n",
       "      <td>7745</td>\n",
       "      <td>552</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>True</td>\n",
       "      <td>0.071272</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>NATI CORRIÓ DROGADA POR ÁMSTERDAM</td>\n",
       "      <td>2024-10-05T12:00:28Z</td>\n",
       "      <td>FJb5YHpWoDI</td>\n",
       "      <td>none</td>\n",
       "      <td>8722</td>\n",
       "      <td>507</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "      <td>0.058129</td>\n",
       "      <td>0.000917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel_name                channel_id  \\\n",
       "0         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "1         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "2         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "3         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "4         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "\n",
       "                                             title          published_at  \\\n",
       "0             BETU ES EL NUEVO REY DE LOS MACARONS  2024-10-05T22:00:33Z   \n",
       "1                  EL PELAO FILOSOFÓ SOBRE EL MATE  2024-10-05T20:00:08Z   \n",
       "2              HOMI ELEVANDO EL NIVEL DEL PROGRAMA  2024-10-05T18:00:11Z   \n",
       "3  EL MITO DEL PETISO PIJUDO EN HABLEMOS SIN SABER  2024-10-05T16:00:16Z   \n",
       "4                NATI CORRIÓ DROGADA POR ÁMSTERDAM  2024-10-05T12:00:28Z   \n",
       "\n",
       "      video_id video_type  view_count  like_count  comment_count  \\\n",
       "0  gtTM9GJ8nZU       none         114          39              0   \n",
       "1  OppCmaXtZME       none         197          54              0   \n",
       "2  Q3LCtQoVFfE       none        3214         327              2   \n",
       "3  J8KDqUj9B2g       none        7745         552              3   \n",
       "4  FJb5YHpWoDI       none        8722         507              8   \n",
       "\n",
       "   duration_seconds  is_short  likes_per_view  comments_per_view  \n",
       "0                58      True        0.342105           0.000000  \n",
       "1                54      True        0.274112           0.000000  \n",
       "2                52      True        0.101742           0.000622  \n",
       "3                58      True        0.071272           0.000387  \n",
       "4                47      True        0.058129           0.000917  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itba-cde-pda-jT53MlBj-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

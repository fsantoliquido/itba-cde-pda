{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'indent' is an invalid keyword argument for print()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     16\u001b[0m request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39msearch()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m     17\u001b[0m     part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     channelId\u001b[38;5;241m=\u001b[39mCHANNEL_ID,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     maxResults\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Puedes ajustar esto seg煤n tus necesidades\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m response \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitems\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'indent' is an invalid keyword argument for print()"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "# Datos de tu proyecto de Google Cloud\n",
    "API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "CHANNEL_ID = 'UC7mJ2EDXFomeDIRFu5FtEbA'\n",
    "\n",
    "# Inicializa el cliente de la API\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# Establecer la fecha desde la cual buscar los videos\n",
    "published_after = datetime(2024, 9, 7).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "# Realiza la consulta a la API para obtener los videos\n",
    "request = youtube.search().list(\n",
    "    part='snippet',\n",
    "    channelId=CHANNEL_ID,\n",
    "    publishedAfter=published_after,\n",
    "    order='date',  # Ordenar por fecha\n",
    "    type='video',  # Asegurarse de que solo busca videos\n",
    "    maxResults=1  # Puedes ajustar esto seg煤n tus necesidades\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "print(json.dumps(response['items']),indent = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"kind\": \"youtube#searchListResponse\",\n",
      "    \"etag\": \"tCj1FOheDEdZdaRqmj7KFqBo1i4\",\n",
      "    \"nextPageToken\": \"CAEQAA\",\n",
      "    \"regionCode\": \"AR\",\n",
      "    \"pageInfo\": {\n",
      "        \"totalResults\": 174,\n",
      "        \"resultsPerPage\": 1\n",
      "    },\n",
      "    \"items\": [\n",
      "        {\n",
      "            \"kind\": \"youtube#searchResult\",\n",
      "            \"etag\": \"tj23L4f2lUL4MojvcHwD2J1RUGs\",\n",
      "            \"id\": {\n",
      "                \"kind\": \"youtube#video\",\n",
      "                \"videoId\": \"YaaskpMWLYY\"\n",
      "            },\n",
      "            \"snippet\": {\n",
      "                \"publishedAt\": \"2024-09-21T12:00:36Z\",\n",
      "                \"channelId\": \"UC7mJ2EDXFomeDIRFu5FtEbA\",\n",
      "                \"title\": \"NATI Y BETU CANCELADOS POR LOS GUSTOS DE HELADO\",\n",
      "                \"description\": \"betular #natijota #eialmoldavsky #homeropettinato #olgaenvivo #olga #streaming #stream #shortclip #shorts.\",\n",
      "                \"thumbnails\": {\n",
      "                    \"default\": {\n",
      "                        \"url\": \"https://i.ytimg.com/vi/YaaskpMWLYY/default.jpg\",\n",
      "                        \"width\": 120,\n",
      "                        \"height\": 90\n",
      "                    },\n",
      "                    \"medium\": {\n",
      "                        \"url\": \"https://i.ytimg.com/vi/YaaskpMWLYY/mqdefault.jpg\",\n",
      "                        \"width\": 320,\n",
      "                        \"height\": 180\n",
      "                    },\n",
      "                    \"high\": {\n",
      "                        \"url\": \"https://i.ytimg.com/vi/YaaskpMWLYY/hqdefault.jpg\",\n",
      "                        \"width\": 480,\n",
      "                        \"height\": 360\n",
      "                    }\n",
      "                },\n",
      "                \"channelTitle\": \"OLGA\",\n",
      "                \"liveBroadcastContent\": \"none\",\n",
      "                \"publishTime\": \"2024-09-21T12:00:36Z\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(response,indent = 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "published_after =  datetime(2024, 9, 15).isoformat(\"T\") + \"Z\"\n",
    "request = youtube.search().list(\n",
    "        part='snippet',\n",
    "        channelId='UC7mJ2EDXFomeDIRFu5FtEbA',\n",
    "        publishedAfter=published_after,\n",
    "        order='date',\n",
    "        type='video',\n",
    "    )\n",
    "\n",
    "response = request.execute()\n",
    "response['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                              title  \\\n",
      "0  2024-09-18                       NOLY Y SU LTIMO DA DE VIDA   \n",
      "1  2024-09-18   MALDICION, VA A SER UN DIA HERMOSO con Mario...   \n",
      "2  2024-09-18  VINO LEILA A BANCAR | #QUROMPIMOS Completo - ...   \n",
      "3  2024-09-18  CUMPLE ALFRE, DEBATE DE LAS ESTRELLAS DEPORTIV...   \n",
      "4  2024-09-18  #EDICIONESPECIAL | MICAELA&#39;S, MEJORES AMIG...   \n",
      "\n",
      "           published_at  \n",
      "0  2024-09-18T10:00:14Z  \n",
      "1  2024-09-18T10:10:28Z  \n",
      "2  2024-09-18T02:02:09Z  \n",
      "3  2024-09-18T00:45:01Z  \n",
      "4  2024-09-18T03:17:34Z  \n"
     ]
    }
   ],
   "source": [
    "from utils import initialize_youtube_api, get_videos_from_channel, transform_video_data, group_videos_by_date, CHANNEL_IDS\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    "    Tiene tres etapas:\n",
    "    iterar por cada channel y traerme la informaci贸n sobre qu茅 videos se publicaron\n",
    "    iterar sobre los videos y traernos las statistics y las metemos en un dataframe\n",
    "    subimos el dataframe a redshift\n",
    "    \n",
    "\"\"\"\n",
    "# Inicializamos la API\n",
    "youtube = initialize_youtube_api()\n",
    "\n",
    "# Establezco la fecha de inicio (煤ltimos N d铆as)\n",
    "published_after = datetime(2024, 9, 15).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "all_videos = []\n",
    "\n",
    "# Para cada channel de youtube itero y me traigo los videos\n",
    "for channel_id in CHANNEL_IDS:\n",
    "    videos = get_videos_from_channel(youtube, channel_id, published_after)\n",
    "    transformed_videos = transform_video_data(videos)\n",
    "    all_videos.extend(transformed_videos)\n",
    "\n",
    "# Agrupo por fecha\n",
    "grouped_videos = group_videos_by_date(all_videos)\n",
    "\n",
    "# Convertir los datos agrupados en una lista de diccionarios para DataFrame\n",
    "video_data = []\n",
    "for date, videos in grouped_videos.items():\n",
    "    for video in videos:\n",
    "        video_data.append({\n",
    "            'date': date,\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at']\n",
    "        })\n",
    "\n",
    "# Guardamos los datos en un DataFrame\n",
    "df = pd.DataFrame(video_data)\n",
    "\n",
    "# Guardar en un archivo CSV\n",
    "df.to_csv('youtube_videos.csv', index=False)\n",
    "\n",
    "# Mostrar los primeros registros del DataFrame para verificar\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Inicializamos la API\u001b[39;00m\n\u001b[1;32m      4\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOUTUBE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m youtube \u001b[38;5;241m=\u001b[39m \u001b[43mbuild\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoutube\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv3\u001b[39m\u001b[38;5;124m'\u001b[39m, developerKey\u001b[38;5;241m=\u001b[39mAPI_KEY)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Realizamos la b煤squeda del canal por handle (nombre del canal)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39msearch()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m      9\u001b[0m     part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     q\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@VorterixOficial\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     maxResults\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Solo necesitamos el canal\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Inicializamos la API\n",
    "API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# Realizamos la b煤squeda del canal por handle (nombre del canal)\n",
    "request = youtube.search().list(\n",
    "    part='snippet',\n",
    "    q='@VorterixOficial',\n",
    "    type='channel',\n",
    "    maxResults=1  # Solo necesitamos el canal\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "# Extraemos el ID del canal de la respuesta\n",
    "for item in response['items']:\n",
    "    channel_id = item['snippet']['channelId']\n",
    "    print(f\"ID del canal: {channel_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from googleapiclient.errors import HttpError\n",
    "import isodate\n",
    "\n",
    "def convert_duration_to_seconds(duration_iso):\n",
    "    # Convierte la duraci贸n ISO 8601 a un objeto timedelta\n",
    "    duration = isodate.parse_duration(duration_iso)\n",
    "    return int(duration.total_seconds())\n",
    "\n",
    "def get_channel_info(youtube, channel_id):\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet,statistics',\n",
    "            id=channel_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extraemos la informaci贸n del canal\n",
    "        channel_info = response['items'][0]\n",
    "        channel_data = {\n",
    "            'channel_name': channel_info['snippet']['title'],\n",
    "            'channel_id': channel_info['id'],\n",
    "            'subscriber_count': channel_info['statistics'].get('subscriberCount', 0)\n",
    "        }\n",
    "\n",
    "        return channel_data\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Error al obtener informaci贸n del canal {channel_id}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri贸 un error inesperado: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_videos_from_channel(youtube, channel_id, published_after, max_requests=10, sleep_time=1):\n",
    "    videos = []\n",
    "    request_count = 0\n",
    "    \n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            channelId=channel_id,\n",
    "            publishedAfter=published_after,\n",
    "            order='date',\n",
    "            type='video',\n",
    "            maxResults=10  # Puedes ajustar seg煤n tus necesidades\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Procesamos la respuesta\n",
    "        for item in response['items']:\n",
    "            video_data = {\n",
    "                'title': item['snippet']['title'],\n",
    "                'published_at': item['snippet']['publishedAt'],\n",
    "                'video_id': item['id']['videoId'],\n",
    "                'video_type': item['snippet'].get('liveBroadcastContent', 'none')  # live, upcoming o none\n",
    "            }\n",
    "            videos.append(video_data)\n",
    "        \n",
    "        request_count += 1\n",
    "        \n",
    "        # Si hemos alcanzado el l铆mite de solicitudes, hacemos un sleep\n",
    "        if request_count % max_requests == 0:\n",
    "            print(f\"Realizadas {request_count} solicitudes. Esperando {sleep_time} segundos para continuar...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Error al obtener videos del canal {channel_id}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri贸 un error inesperado: {e}\")\n",
    "\n",
    "    return videos\n",
    "\n",
    "def get_video_statistics(youtube, video_ids, max_requests=10, sleep_time=1):\n",
    "    video_stats = []\n",
    "    request_count = 0\n",
    "    \n",
    "    try:\n",
    "        stats_request = youtube.videos().list(\n",
    "            part='statistics,contentDetails',\n",
    "            id=','.join(video_ids)  # Pasamos los IDs de los videos como una lista separada por comas\n",
    "        )\n",
    "        stats_response = stats_request.execute()\n",
    "\n",
    "        # Procesamos las estad铆sticas\n",
    "        for stats in stats_response['items']:\n",
    "            # Convertimos la duraci贸n de ISO 8601 a segundos\n",
    "            duration_iso = stats['contentDetails']['duration']\n",
    "            duration_seconds = convert_duration_to_seconds(duration_iso)\n",
    "            \n",
    "            # Verificamos si el video es un YouTube Short (menos de 60 segundos)\n",
    "            is_short = duration_seconds < 60\n",
    "            \n",
    "            stats_data = {\n",
    "                'video_id': stats['id'],\n",
    "                'view_count': stats['statistics'].get('viewCount', 0),\n",
    "                'like_count': stats['statistics'].get('likeCount', 0),\n",
    "                'comment_count': stats['statistics'].get('commentCount', 0),\n",
    "                'duration': duration_seconds,\n",
    "                'is_short': is_short\n",
    "            }\n",
    "            video_stats.append(stats_data)\n",
    "        \n",
    "        request_count += 1\n",
    "\n",
    "        # Si hemos alcanzado el l铆mite de solicitudes, hacemos un sleep\n",
    "        if request_count % max_requests == 0:\n",
    "            print(f\"Realizadas {request_count} solicitudes. Esperando {sleep_time} segundos para continuar...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Error al obtener estad铆sticas de videos: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri贸 un error inesperado: {e}\")\n",
    "\n",
    "    return video_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtenemos la informaci贸n del canal\n",
    "channel_info = get_channel_info(youtube, channel_id)\n",
    "\n",
    "\n",
    "# Obtener la fecha de consulta actual\n",
    "consulta_fecha = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Crear un DataFrame con la informaci贸n de suscriptores del canal\n",
    "df_subscribers = pd.DataFrame([{\n",
    "    'channel_name': channel_info['channel_name'],\n",
    "    'channel_id': channel_info['channel_id'],\n",
    "    'consulta_fecha': consulta_fecha,\n",
    "    'subscriber_count': channel_info['subscriber_count']\n",
    "}])\n",
    "\n",
    "# Verificamos que se haya obtenido la informaci贸n del canal\n",
    "if channel_info:\n",
    "    # Creamos una lista vac铆a para almacenar los datos combinados\n",
    "    combined_data = []\n",
    "\n",
    "    # Iteramos sobre los videos y sus estad铆sticas\n",
    "    for video, stats in zip(videos, video_stats):\n",
    "        # Combinamos la informaci贸n de los videos, las estad铆sticas y el canal en un solo diccionario\n",
    "        combined_data.append({\n",
    "            'channel_name': channel_info['channel_name'],\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at'],\n",
    "            'video_id': video['video_id'],\n",
    "            'video_type': video['video_type'],\n",
    "            'view_count': stats['view_count'],\n",
    "            'like_count': stats['like_count'],\n",
    "            'comment_count': stats['comment_count'],\n",
    "            'duration_seconds': stats['duration'],\n",
    "            'is_short': stats['is_short']\n",
    "        })\n",
    "\n",
    "    # Convertimos la lista de diccionarios en un DataFrame de pandas\n",
    "    df_videos = pd.DataFrame(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_type</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>is_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>MIRANDA Y UN TEMAZO DE CRISTIAN CASTRO</td>\n",
       "      <td>2024-09-21T14:00:35Z</td>\n",
       "      <td>XjEX99nbf6Q</td>\n",
       "      <td>none</td>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>NATI Y BETU CANCELADOS POR LOS GUSTOS DE HELADO</td>\n",
       "      <td>2024-09-21T12:00:36Z</td>\n",
       "      <td>YaaskpMWLYY</td>\n",
       "      <td>none</td>\n",
       "      <td>573</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>GREGO DE CHETO A HUMILDE POR UNA TORTA FRITA</td>\n",
       "      <td>2024-09-21T10:00:15Z</td>\n",
       "      <td>HzU45a7Pxjc</td>\n",
       "      <td>none</td>\n",
       "      <td>1135</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>OLIVIA FIRPO CUMPLI SU SUEO EN EL TEATRO COLN</td>\n",
       "      <td>2024-09-21T00:19:15Z</td>\n",
       "      <td>F7ZejuxyrDw</td>\n",
       "      <td>none</td>\n",
       "      <td>4344</td>\n",
       "      <td>273</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>CARO Y SOFI GONET DEL ODIO AL AMOR POR LA MODA</td>\n",
       "      <td>2024-09-20T20:00:17Z</td>\n",
       "      <td>ma15lzAoKx4</td>\n",
       "      <td>none</td>\n",
       "      <td>1498</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>AGUANTE OEISIS | So帽茅 que Volaba | COMPLETO 20/9</td>\n",
       "      <td>2024-09-20T18:07:37Z</td>\n",
       "      <td>OZeLwvFPfGk</td>\n",
       "      <td>none</td>\n",
       "      <td>56980</td>\n",
       "      <td>1649</td>\n",
       "      <td>182</td>\n",
       "      <td>8283</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>RECHI Y EL MEJOR CHISTE DE LA HISTORIA</td>\n",
       "      <td>2024-09-20T18:00:26Z</td>\n",
       "      <td>i3YkcpDXcP4</td>\n",
       "      <td>none</td>\n",
       "      <td>11893</td>\n",
       "      <td>769</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>ESPECIAL SUPERCLSICO: BOCA - RIVER en OLGA co...</td>\n",
       "      <td>2024-09-20T17:52:47Z</td>\n",
       "      <td>XXqVT9gaw_E</td>\n",
       "      <td>upcoming</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>GIME SE PELE CON TODOS</td>\n",
       "      <td>2024-09-20T16:00:45Z</td>\n",
       "      <td>gFhK3npHMRo</td>\n",
       "      <td>none</td>\n",
       "      <td>2244</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>INTELIGENCIA ARTIFICIAL y la PREVIA del SUPERC...</td>\n",
       "      <td>2024-09-20T15:24:24Z</td>\n",
       "      <td>5xcXpVf3IPU</td>\n",
       "      <td>none</td>\n",
       "      <td>86932</td>\n",
       "      <td>2380</td>\n",
       "      <td>149</td>\n",
       "      <td>7261</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel_name                channel_id  \\\n",
       "0         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "1         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "2         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "3         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "4         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "5         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "6         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "7         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "8         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "9         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "\n",
       "                                               title          published_at  \\\n",
       "0             MIRANDA Y UN TEMAZO DE CRISTIAN CASTRO  2024-09-21T14:00:35Z   \n",
       "1    NATI Y BETU CANCELADOS POR LOS GUSTOS DE HELADO  2024-09-21T12:00:36Z   \n",
       "2       GREGO DE CHETO A HUMILDE POR UNA TORTA FRITA  2024-09-21T10:00:15Z   \n",
       "3   OLIVIA FIRPO CUMPLI SU SUEO EN EL TEATRO COLN  2024-09-21T00:19:15Z   \n",
       "4     CARO Y SOFI GONET DEL ODIO AL AMOR POR LA MODA  2024-09-20T20:00:17Z   \n",
       "5   AGUANTE OEISIS | So帽茅 que Volaba | COMPLETO 20/9  2024-09-20T18:07:37Z   \n",
       "6             RECHI Y EL MEJOR CHISTE DE LA HISTORIA  2024-09-20T18:00:26Z   \n",
       "7  ESPECIAL SUPERCLSICO: BOCA - RIVER en OLGA co...  2024-09-20T17:52:47Z   \n",
       "8                            GIME SE PELE CON TODOS  2024-09-20T16:00:45Z   \n",
       "9  INTELIGENCIA ARTIFICIAL y la PREVIA del SUPERC...  2024-09-20T15:24:24Z   \n",
       "\n",
       "      video_id video_type view_count like_count comment_count  \\\n",
       "0  XjEX99nbf6Q       none         35         15             0   \n",
       "1  YaaskpMWLYY       none        573        127             1   \n",
       "2  HzU45a7Pxjc       none       1135         83             0   \n",
       "3  F7ZejuxyrDw       none       4344        273             2   \n",
       "4  ma15lzAoKx4       none       1498        103             0   \n",
       "5  OZeLwvFPfGk       none      56980       1649           182   \n",
       "6  i3YkcpDXcP4       none      11893        769             6   \n",
       "7  XXqVT9gaw_E   upcoming          0          8             0   \n",
       "8  gFhK3npHMRo       none       2244        202             0   \n",
       "9  5xcXpVf3IPU       none      86932       2380           149   \n",
       "\n",
       "   duration_seconds  is_short  \n",
       "0                36      True  \n",
       "1                60     False  \n",
       "2                57      True  \n",
       "3                55      True  \n",
       "4                60     False  \n",
       "5              8283     False  \n",
       "6                60     False  \n",
       "7                 0      True  \n",
       "8                59      True  \n",
       "9              7261     False  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos['likes_per_view'] = df_videos['like_count'] / df_videos['view_count']\n",
    "df_videos['comments_per_view'] = df_videos['comment_count'] / df_videos['view_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import initialize_youtube_api, get_videos_from_channel, get_video_statistics, get_channel_info, group_videos_by_date, CHANNEL_IDS\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "    Tiene tres etapas:\n",
    "    1. Iterar por cada channel y traer la informaci贸n sobre qu茅 videos se publicaron\n",
    "    2. Iterar sobre los videos y traernos las estad铆sticas y las metemos en un DataFrame\n",
    "    3. Subir los DataFrames (videos y suscriptores) a Redshift o guardarlos en CSV\n",
    "    \n",
    "    WARNING: Este c贸digo es para correr de manera diaria.\n",
    "    Una vulnerabilidad que tiene es que la tabla con cantidad de suscriptores si se corre desde una fecha en particular, no va a obtener los subs desde esa fecha, si no de manera diaria\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Inicializamos la API\n",
    "youtube = initialize_youtube_api()\n",
    "\n",
    "# Establezco la fecha de inicio (煤ltimos N d铆as)\n",
    "published_after = (datetime.now() - timedelta(days=7)).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "all_videos = []\n",
    "all_video_stats = []\n",
    "df_subscribers_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Se itera para cada channel de YouTube y sacamos estad铆sticas: cantindad de subs a la fecha de consulta, sus videos y para cada video m茅tricas de views, comentarios y likes\n",
    "for channel_id in CHANNEL_IDS:\n",
    "    # Obtenemos la informaci贸n del canal (nombre, id, suscriptores)\n",
    "    channel_info = get_channel_info(youtube, channel_id)\n",
    "    \n",
    "    # Obtener la fecha de consulta actual\n",
    "    consulta_fecha = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Crear un df con la info de suscriptores del canal\n",
    "    df_subscribers_list.append({\n",
    "        'channel_name': channel_info['channel_name'],\n",
    "        'channel_id': channel_info['channel_id'],\n",
    "        'consulta_fecha': consulta_fecha,\n",
    "        'subscriber_count': channel_info['subscriber_count']\n",
    "    })\n",
    "\n",
    "    videos = get_videos_from_channel(youtube, channel_id, published_after)\n",
    "    \n",
    "    # Filtramos videos que contengan alguno de los campos que necesitamos para obtener el video ID. \n",
    "    # Hacemos esto por si cambia el nombre de la key con la que se guarda el video_id. \n",
    "    video_ids = []\n",
    "    for video in videos:\n",
    "        if 'video_id' in video:\n",
    "            video_ids.append(video['video_id'])\n",
    "        elif 'id' in video and isinstance(video['id'], dict) and 'videoId' in video['id']:\n",
    "            video_ids.append(video['id']['videoId'])\n",
    "        else:\n",
    "            # Si no se encuentra el video_id, imprimimos el video para depuraci贸n\n",
    "            print(f\"Video sin 'video_id' o 'id' v谩lido detectado: {video}\")\n",
    "    \n",
    "    # Las stats de los videos\n",
    "    video_stats = get_video_statistics(youtube, video_ids)\n",
    "    \n",
    "    # Agregamos los videos y las estad铆sticas a una lista combinada\n",
    "    for video, stats in zip(videos, video_stats):\n",
    "        video_id = video.get('video_id') or (video['id']['videoId'] if isinstance(video['id'], dict) and 'videoId' in video['id'] else 'N/A')\n",
    "        \n",
    "        all_videos.append({\n",
    "            'channel_name': channel_info['channel_name'],\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at'],\n",
    "            'video_id': video['video_id'],\n",
    "            'video_type': video['video_type'],\n",
    "            'view_count': stats['view_count'],\n",
    "            'like_count': stats['like_count'],\n",
    "            'comment_count': stats['comment_count'],\n",
    "            'duration_seconds': stats['duration'],\n",
    "            'is_short': stats['is_short']\n",
    "        })\n",
    "\n",
    "# Pasamos a un df los videos\n",
    "df_videos = pd.DataFrame(all_videos)\n",
    "\n",
    "# Hago una transformaci贸n de los datos \n",
    "df_videos['like_count'] = pd.to_numeric(df_videos['like_count'], errors='coerce')\n",
    "df_videos['view_count'] = pd.to_numeric(df_videos['view_count'], errors='coerce')\n",
    "df_videos['comment_count'] = pd.to_numeric(df_videos['comment_count'], errors='coerce')\n",
    "\n",
    "\n",
    "df_videos['likes_per_view'] = df_videos['like_count'] / df_videos['view_count']\n",
    "df_videos['comments_per_view'] = df_videos['comment_count'] / df_videos['view_count']\n",
    "\n",
    "# Pasamos a un df los logs de suscriptores\n",
    "df_subscribers = pd.DataFrame(df_subscribers_list)\n",
    "\n",
    "\n",
    "\n",
    "# Guardar los DataFrames en archivos CSV\n",
    "df_videos.to_csv('youtube_videos.csv', index=False)\n",
    "df_subscribers.to_csv('youtube_subscribers_history.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7_/ydy5jwh51735b2j1dqgskh7c0000gn/T/ipykernel_6267/2684085640.py:14: SADeprecationWarning: The dbapi() classmethod on dialect classes has been renamed to import_dbapi().  Implement an import_dbapi() classmethod directly on class <class 'sqlalchemy_redshift.dialect.RedshiftDialect_psycopg2'> to remove this warning; the old .dbapi() classmethod may be maintained for backwards compatibility.\n",
      "  engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Credenciales de conexi贸n a Redshift\n",
    "DATABASE_TYPE = 'redshift+psycopg2'\n",
    "DBAPI = 'psycopg2'\n",
    "ENDPOINT = 'redshift-pda-cluster.cnuimntownzt.us-east-2.redshift.amazonaws.com'\n",
    "USER = '2024_franco_santoliquido'\n",
    "PASSWORD = 'L6^&9!2$xQ'\n",
    "PORT = 5439\n",
    "DATABASE = 'pda'\n",
    "\n",
    "# Cadena de conexi贸n a Redshift\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos.to_sql('2024_franco_santoliquido_schema.youtube_videos_stg', engine, index=False, if_exists='replace')\n",
    "df_subscribers.to_sql('2024_franco_santoliquido_schema.youtube_subscribers_stg', engine, index=False, if_exists='replace') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chili Parker: Hay que autoinmolarse contra las ideas de la izquierda por el futuro de nuestros hijos',\n",
       "  'published_at': '2024-09-22T16:27:42Z',\n",
       "  'video_id': 'g1W6OEL2bW4',\n",
       "  'video_type': 'none'},\n",
       " {'title': ' CHILI PARKER EN LA MISA  LA MORCILLA CON BCULO y CHADMANZIO | La Misa de Dan',\n",
       "  'published_at': '2024-09-21T02:26:15Z',\n",
       "  'video_id': 'KGkJcg5FULQ',\n",
       "  'video_type': 'none'},\n",
       " {'title': 'ZURDO QUEDA EN RIDCULO #lamisa #podcast #milei',\n",
       "  'published_at': '2024-09-20T22:45:16Z',\n",
       "  'video_id': 'KQf8hZj3P3U',\n",
       "  'video_type': 'none'},\n",
       " {'title': 'ZURDOS UNIVERSITARIOS QUEDAN EN RIDCULO',\n",
       "  'published_at': '2024-09-20T21:00:07Z',\n",
       "  'video_id': 'cUHm5TYjfLg',\n",
       "  'video_type': 'none'},\n",
       " {'title': '驴MEJOR PRESIDENTE? LO QUE NADIE TE CONT DE JULIO.A.ROCA',\n",
       "  'published_at': '2024-09-20T19:00:06Z',\n",
       "  'video_id': 'bV-w9Z1SS4g',\n",
       "  'video_type': 'none'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Obtenemos la informaci贸n del canal\n",
    "channel_info = get_channel_info(youtube, channel_id)\n",
    "\n",
    "# Verificamos que se haya obtenido la informaci贸n del canal\n",
    "if channel_info:\n",
    "    # Creamos una lista vac铆a para almacenar los datos combinados\n",
    "    combined_data = []\n",
    "\n",
    "    # Iteramos sobre los videos y sus estad铆sticas\n",
    "    for video, stats in zip(videos, video_stats):\n",
    "        # Combinamos la informaci贸n de los videos, las estad铆sticas y el canal en un solo diccionario\n",
    "        combined_data.append({\n",
    "            'channel_name': channel_info['channel_name'],\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at'],\n",
    "            'video_id': video['video_id'],\n",
    "            'video_type': video['video_type'],\n",
    "            'view_count': stats['view_count'],\n",
    "            'like_count': stats['like_count'],\n",
    "            'comment_count': stats['comment_count'],\n",
    "            'duration_seconds': stats['duration'],\n",
    "            'is_short': stats['is_short']\n",
    "        })\n",
    "\n",
    "    # Convertimos la lista de diccionarios en un DataFrame de pandas\n",
    "    df_videos = pd.DataFrame(combined_data)\n",
    "\n",
    "\n",
    "df['likes_per_view'] = df['like_count']/df['view_count']\n",
    "df['comments_per_view'] = df['comment_count'] / df['view_count']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.errors import HttpError\n",
    "import isodate\n",
    "\n",
    "\n",
    "# Lista de canales a monitorear\n",
    "CHANNEL_IDS = [\n",
    "    'UC7mJ2EDXFomeDIRFu5FtEbA', \n",
    "    'UCvCTWHCbBC0b9UIeLeNs8ug',\n",
    "    'UCWSfXECGo1qK_H7SXRaUSMg',\n",
    "    'UCTHaNTsP7hsVgBxARZTuajw',\n",
    "    'UC4mdhKZXjrKoq5aVG6juHEg' \n",
    "]\n",
    "\n",
    "\n",
    "#Tres funciones para subir a redshift: una conecta, la otra uploadea en raw y la otra hace el upsert\n",
    "def connect_to_redshift():\n",
    "\n",
    "    DATABASE_TYPE = 'redshift+psycopg2'\n",
    "    DBAPI = 'psycopg2'\n",
    "    ENDPOINT = os.getenv('REDSHIFT_ENDPOINT')\n",
    "    USER = os.getenv('REDSHIFT_USER')        \n",
    "    PASSWORD = os.getenv('REDSHIFT_PASSWORD')\n",
    "    PORT = 5439\n",
    "    DATABASE = os.getenv('REDSHIFT_DATABASE')\n",
    "    engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "    return engine\n",
    "\n",
    "def upload_to_redshift(engine, df, destintation_table, schema):\n",
    "    df.to_sql(destintation_table, engine, schema, index=False, if_exists='replace')\n",
    "\n",
    "def run_sql_queries(engine):\n",
    "    # Obtener la ruta absoluta del archivo queries.sql\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))  # Obtiene la ruta absoluta de module_etl\n",
    "    queries_file_path = os.path.join(base_dir, 'queries.sql')  # Se une para llegar a /queries.sql\n",
    "    \n",
    "    if not os.path.exists(queries_file_path):\n",
    "        raise FileNotFoundError(f\"No se encontr贸 el archivo queries.sql en la ruta: {queries_file_path}\")\n",
    "    \n",
    "    with open(queries_file_path, 'r') as file:\n",
    "        sql_queries = file.read()\n",
    "    queries = sql_queries.split(';')\n",
    "    \n",
    "    # Ejecuto las queries del archivo queries que tiene el upsert para hacer\n",
    "    with engine.connect() as connection:\n",
    "        for query in queries:\n",
    "            query = query.strip()\n",
    "            if query:  # Si la consulta no est谩 vac铆a\n",
    "                connection.execute(query)\n",
    "                print(f\"Ejecutada la consulta:\\n{query}\\n\")\n",
    "\n",
    "\n",
    "# Convierto duracion que da Youtube ISO 8601 a un objeto de tiempo\n",
    "def convert_duration_to_seconds(duration_iso):\n",
    "\n",
    "    duration = isodate.parse_duration(duration_iso)\n",
    "    return int(duration.total_seconds())\n",
    "\n",
    "# Inicio la api de Youtube con mis credenciales\n",
    "def initialize_youtube_api():\n",
    "    API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "    return youtube\n",
    "\n",
    "\n",
    "# Funcion para buscar videos de un canal, con retrys\n",
    "\n",
    "def get_videos_from_channel(youtube, channel_id, published_after, max_requests=10, sleep_time=1, max_retries=3):\n",
    "    videos = []\n",
    "    request_count = 0\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            request = youtube.search().list(\n",
    "                part='snippet',\n",
    "                channelId=channel_id,\n",
    "                publishedAfter=published_after,\n",
    "                order='date',\n",
    "                type='video'\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # De la respuesta sacamos titulo, momento de publicacion, id del video y tipo de video (live, upcoming o none)\n",
    "            for item in response['items']:\n",
    "                video_data = {\n",
    "                    'title': item['snippet']['title'],\n",
    "                    'published_at': item['snippet']['publishedAt'],\n",
    "                    'video_id': item['id']['videoId'],\n",
    "                    'video_type': item['snippet'].get('liveBroadcastContent', 'none')  # live, upcoming o none\n",
    "                }\n",
    "                videos.append(video_data)\n",
    "            \n",
    "            request_count += 1\n",
    "            \n",
    "            # Si se alcanza el l铆mite de requests, metemos un sleep comentado\n",
    "            if request_count % max_requests == 0:\n",
    "                print(f\"Realizadas {request_count} solicitudes. Esperando {sleep_time} segundos para continuar...\")\n",
    "                time.sleep(sleep_time)\n",
    "            \n",
    "            return videos\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"Error al obtener videos del canal {channel_id}: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurri贸 un error inesperado: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        \n",
    "        # Meto otro sleep para retry\n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "    raise Exception(f\"No se pudo obtener los videos del canal {channel_id} despu茅s de {max_retries} intentos.\")\n",
    "\n",
    "#Busca la informaci贸n del canal de Youtube que le pases\n",
    "def get_channel_info(youtube, channel_id, max_retries=3, sleep_time=1):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            request = youtube.channels().list(\n",
    "                part='snippet,statistics',\n",
    "                id=channel_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # la respuesta tiene una key \"items\" contiene la data del canal y la cantidad de suscriptores\n",
    "            channel_info = response['items'][0]\n",
    "            channel_data = {\n",
    "                'channel_name': channel_info['snippet']['title'],\n",
    "                'channel_id': channel_info['id'],\n",
    "                'subscriber_count': channel_info['statistics'].get('subscriberCount', 0)\n",
    "            }\n",
    "\n",
    "            return channel_data\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"Error al obtener informaci贸n del canal {channel_id}: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurri贸 un error inesperado: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        \n",
    "        # Esperamos antes de intentar nuevamente\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    # Tiro el error para que pinche si no puedo obtener la informaci贸n del canal\n",
    "    raise Exception(f\"No se pudo obtener la informaci贸n del canal {channel_id} despu茅s de {max_retries} intentos.\")\n",
    "\n",
    "def get_video_statistics(youtube, video_ids, max_requests=10, sleep_time=2, max_retries=3):\n",
    "    video_stats = []\n",
    "    request_count = 0\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Pasamos los IDs de los videos como una lista\n",
    "            stats_request = youtube.videos().list(\n",
    "                part='statistics,contentDetails',\n",
    "                id=','.join(video_ids)\n",
    "            )\n",
    "            stats_response = stats_request.execute()\n",
    "\n",
    "            # Procesamos las estad铆sticas\n",
    "            for stats in stats_response['items']:\n",
    "                duration_iso = stats['contentDetails'].get('duration', None)  # Ajuste aqu铆\n",
    "                if duration_iso is None:\n",
    "                    print(f\"El video {stats['id']} no tiene duraci贸n, omitimos el dato.\")\n",
    "                    continue \n",
    "\n",
    "                duration_seconds = convert_duration_to_seconds(duration_iso)\n",
    "                \n",
    "                # Verificamos si el video es un YouTube Short (menos de 60 segundos)\n",
    "                is_short = duration_seconds < 60\n",
    "                \n",
    "                stats_data = {\n",
    "                    'video_id': stats['id'],\n",
    "                    'view_count': stats['statistics'].get('viewCount', 0),\n",
    "                    'like_count': stats['statistics'].get('likeCount', 0),\n",
    "                    'comment_count': stats['statistics'].get('commentCount', 0),\n",
    "                    'duration': duration_seconds,\n",
    "                    'is_short': is_short\n",
    "                }\n",
    "                video_stats.append(stats_data)\n",
    "            \n",
    "            request_count += 1\n",
    "\n",
    "            # Si se llega el l铆mite de solicitudes, hacemos un sleep\n",
    "            if request_count % max_requests == 0:\n",
    "                print(f\"Realizadas {request_count} solicitudes. Esperando {sleep_time} segundos para continuar...\")\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "            return video_stats\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"Error al obtener estad铆sticas de videos: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Ocurri贸 un error de clave: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurri贸 un error inesperado: {e}. Intento {attempt+1} de {max_retries}\")\n",
    "        \n",
    "        # Esperamos para intentar nuevamente\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    # Tiro el error para que pinche si no puedo obtener las estad铆sticas del canal\n",
    "    raise Exception(f\"No se pudieron obtener las estad铆sticas de videos despu茅s de {max_retries} intentos.\")\n",
    "\n",
    "\n",
    "\n",
    "# Funci贸n para sacar del json el datos que necesito\n",
    "def transform_video_data(videos):\n",
    "    transformed_data = []\n",
    "    for video in videos:\n",
    "        title = video['snippet']['title']\n",
    "        published_at = video['snippet']['publishedAt']\n",
    "        transformed_data.append({\n",
    "            'title': title,\n",
    "            'published_at': published_at\n",
    "        })\n",
    "    return transformed_data\n",
    "\n",
    "# Funci贸n para agrupar datos por dia \n",
    "def group_videos_by_date(videos):\n",
    "    grouped_data = {}\n",
    "    for video in videos:\n",
    "        date = video['published_at'].split(\"T\")[0]\n",
    "        if date not in grouped_data:\n",
    "            grouped_data[date] = []\n",
    "        grouped_data[date].append(video)\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "    Tiene tres etapas:\n",
    "    1. Iterar por cada channel y traer la informaci贸n sobre qu茅 videos se publicaron\n",
    "    2. Iterar sobre los videos y traernos las estad铆sticas y las metemos en un DataFrame\n",
    "    3. Subir los DataFrames (videos y suscriptores) a Redshift o guardarlos en CSV\n",
    "    \n",
    "    WARNING: Este c贸digo es para correr de manera diaria.\n",
    "    Una vulnerabilidad que tiene es que la tabla con cantidad de suscriptores si se corre desde una fecha en particular, no va a obtener los subs desde esa fecha, si no de manera diaria\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Inicializamos la API\n",
    "youtube = initialize_youtube_api()\n",
    "\n",
    "# Establezco la fecha de inicio (煤ltimos N d铆as)\n",
    "published_after = (datetime.now() - timedelta(days=7)).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "all_videos = []\n",
    "all_video_stats = []\n",
    "df_subscribers_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Se itera para cada channel de YouTube y sacamos estad铆sticas: cantindad de subs a la fecha de consulta, sus videos y para cada video m茅tricas de views, comentarios y likes\n",
    "for channel_id in CHANNEL_IDS:\n",
    "    # Obtenemos la informaci贸n del canal (nombre, id, suscriptores)\n",
    "    channel_info = get_channel_info(youtube, channel_id)\n",
    "    \n",
    "    # Obtener la fecha de consulta actual\n",
    "    consulta_fecha = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Crear un df con la info de suscriptores del canal\n",
    "    df_subscribers_list.append({\n",
    "        'channel_name': channel_info['channel_name'],\n",
    "        'channel_id': channel_info['channel_id'],\n",
    "        'consulta_fecha': consulta_fecha,\n",
    "        'subscriber_count': channel_info['subscriber_count']\n",
    "    })\n",
    "\n",
    "    videos = get_videos_from_channel(youtube, channel_id, published_after)\n",
    "    \n",
    "    # Filtramos videos que contengan alguno de los campos que necesitamos para obtener el video ID. \n",
    "    # Hacemos esto por si cambia el nombre de la key con la que se guarda el video_id. \n",
    "    video_ids = []\n",
    "    for video in videos:\n",
    "        if 'video_id' in video:\n",
    "            video_ids.append(video['video_id'])\n",
    "        elif 'id' in video and isinstance(video['id'], dict) and 'videoId' in video['id']:\n",
    "            video_ids.append(video['id']['videoId'])\n",
    "        else:\n",
    "            # Si no se encuentra el video_id, imprimimos el video para depuraci贸n\n",
    "            print(f\"Video sin 'video_id' o 'id' v谩lido detectado: {video}\")\n",
    "    \n",
    "    # Las stats de los videos\n",
    "    video_stats = get_video_statistics(youtube, video_ids)\n",
    "    \n",
    "    # Agregamos los videos y las estad铆sticas a una lista combinada\n",
    "    for video, stats in zip(videos, video_stats):\n",
    "        video_id = video.get('video_id') or (video['id']['videoId'] if isinstance(video['id'], dict) and 'videoId' in video['id'] else 'N/A')\n",
    "        \n",
    "        all_videos.append({\n",
    "            'channel_name': channel_info['channel_name'],\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'title': video['title'],\n",
    "            'published_at': video['published_at'],\n",
    "            'video_id': video['video_id'],\n",
    "            'video_type': video['video_type'],\n",
    "            'view_count': stats['view_count'],\n",
    "            'like_count': stats['like_count'],\n",
    "            'comment_count': stats['comment_count'],\n",
    "            'duration_seconds': stats['duration'],\n",
    "            'is_short': stats['is_short']\n",
    "        })\n",
    "\n",
    "# Pasamos a un df los videos\n",
    "df_videos = pd.DataFrame(all_videos)\n",
    "\n",
    "# Hago una transformaci贸n de los datos \n",
    "df_videos['like_count'] = pd.to_numeric(df_videos['like_count'], errors='coerce')\n",
    "df_videos['view_count'] = pd.to_numeric(df_videos['view_count'], errors='coerce')\n",
    "df_videos['comment_count'] = pd.to_numeric(df_videos['comment_count'], errors='coerce')\n",
    "\n",
    "\n",
    "df_videos['likes_per_view'] = df_videos['like_count'] / df_videos['view_count']\n",
    "df_videos['comments_per_view'] = df_videos['comment_count'] / df_videos['view_count']\n",
    "\n",
    "# Pasamos a un df los logs de suscriptores\n",
    "df_subscribers = pd.DataFrame(df_subscribers_list)\n",
    "\n",
    "#Iniciamos el engine\n",
    "#engine_rs = connect_to_redshift()\n",
    "\n",
    "#Inserto la raw data\n",
    "#upload_to_redshift(engine_rs, df_videos, 'pda.2024_franco_santoliquido_schema.youtube_videos_stg')\n",
    "#upload_to_redshift(engine_rs, df_subscribers, 'pda.2024_franco_santoliquido_schema.youtube_subscribers_stg')\n",
    "\n",
    "#Corro lo que est谩 en el file queries.sql que tiene el upsert a la tabla final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_redshift():\n",
    "    DATABASE_TYPE = 'redshift+psycopg2'\n",
    "    DBAPI = 'psycopg2'\n",
    "    ENDPOINT = 'redshift-pda-cluster.cnuimntownzt.us-east-2.redshift.amazonaws.com'\n",
    "    USER = '2024_franco_santoliquido'\n",
    "    PASSWORD = 'L6^&9!2$xQ'\n",
    "    PORT = 5439\n",
    "    DATABASE = 'pda'\n",
    "\n",
    "    engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "    return engine\n",
    "def upload_to_redshift(engine, df, destintation_table, schema):\n",
    "    df.to_sql(destintation_table, engine, schema, index=False, if_exists='replace')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejecutar las queries directamente usando SQLAlchemy\n",
    "queries = \"\"\"\n",
    "-- Update e insert para la tabla de videos\n",
    "UPDATE pda.\"2024_franco_santoliquido_schema\".BT_YOUTUBE_VIDEO_STATS DW\n",
    "SET \n",
    "    YT_VIEW_COUNT_7D = STG.view_count,\n",
    "    YT_LIKE_COUNT_7D = STG.like_count,\n",
    "    YT_COMMENT_COUNT_7D = STG.comment_count,\n",
    "    AUD_UPD_ID = CURRENT_DATE,\n",
    "    AUD_UPDATED_FROM = 'ETL_YOUTUBE_CDE_PDA'\n",
    "FROM \n",
    "    pda.\"2024_franco_santoliquido_schema\".youtube_videos_stg STG\n",
    "WHERE  \n",
    "    STG.video_id = DW.YT_VIDEO_ID\n",
    "    AND STG.published_at >= DATEADD(day, -7, CURRENT_DATE)\n",
    "    ;\n",
    "\n",
    "INSERT INTO pda.\"2024_franco_santoliquido_schema\".BT_YOUTUBE_VIDEO_STATS (\n",
    "    YT_CHANNEL_NAME,\n",
    "    YT_CHANNEL_ID,\n",
    "    YT_TITLE_NAME,\n",
    "    YT_DATE_PUBLISHED,\n",
    "    YT_VIDEO_ID,\n",
    "    YT_VIDEO_TYPE,\n",
    "    YT_VIEW_COUNT_7D,\n",
    "    YT_LIKE_COUNT_7D,\n",
    "    YT_COMMENT_COUNT_7D,\n",
    "    YT_DURATION_SECS,\n",
    "    YT_IS_SHORT_FLAG,\n",
    "    AUD_UPD_ID,\n",
    "    AUD_INS_DATE,\n",
    "    AUD_UPDATED_FROM\n",
    ")\n",
    "SELECT \n",
    "    STG.channel_name,\n",
    "    STG.channel_id,\n",
    "    STG.title,\n",
    "    STG.published_at,\n",
    "    STG.video_id,\n",
    "    STG.video_type,\n",
    "    STG.view_count,\n",
    "    STG.like_count,\n",
    "    STG.comment_count,\n",
    "    STG.duration_seconds,\n",
    "    STG.is_short,\n",
    "    CURRENT_DATE,\n",
    "    CURRENT_DATE, \n",
    "    'ETL_YOUTUBE_CDE_PDA'\n",
    "FROM \n",
    "    pda.\"2024_franco_santoliquido_schema\".youtube_videos_stg STG\n",
    "LEFT JOIN \n",
    "    pda.\"2024_franco_santoliquido_schema\".BT_YOUTUBE_VIDEO_STATS DW\n",
    "    ON STG.video_id = DW.YT_VIDEO_ID\n",
    "WHERE \n",
    "    DW.YT_VIDEO_ID IS NULL;\n",
    "\n",
    "-- Insert para la tabla de suscriptores\n",
    "\n",
    "INSERT INTO pda.\"2024_franco_santoliquido_schema\".LG_CHANNEL_SUBSCRIBERS (\n",
    "    YT_CHANNEL_NAME,\n",
    "    YT_CHANNEL_ID,\n",
    "    YT_DATE_ID,\n",
    "    YT_SUSCRIBER_COUNT,\n",
    "    AUD_INS_DATE,\n",
    "    AUD_UPDATED_FROM\n",
    ")\n",
    "SELECT \n",
    "    STG.channel_name,\n",
    "    STG.channel_id,\n",
    "    STG.consulta_fecha,\n",
    "    STG.subscriber_count,\n",
    "    CURRENT_DATE,\n",
    "    'ETL_YOUTUBE_CDE_PDA'\n",
    "FROM \n",
    "    pda.\"2024_franco_santoliquido_schema\".youtube_subscribers_stg STG\n",
    "LEFT JOIN \n",
    "    pda.\"2024_franco_santoliquido_schema\".LG_CHANNEL_SUBSCRIBERS DW\n",
    "ON \n",
    "    STG.channel_id = DW.YT_CHANNEL_ID\n",
    "    AND STG.consulta_fecha = DW.YT_DATE_ID\n",
    "WHERE \n",
    "    DW.YT_CHANNEL_ID IS NULL\n",
    "    ;\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que ya tienes tus DataFrames creados (df_videos y df_subscribers):\n",
    "# Sube los DataFrames con el nombre del esquema ajustado\n",
    "\n",
    "upload_to_redshift(engine, df_videos, 'youtube_videos_stg', schema='2024_franco_santoliquido_schema' )\n",
    "upload_to_redshift(engine, df_videos, 'youtube_subscribers_stg', schema='2024_franco_santoliquido_schema' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_type</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>is_short</th>\n",
       "      <th>likes_per_view</th>\n",
       "      <th>comments_per_view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>BETU ES EL NUEVO REY DE LOS MACARONS</td>\n",
       "      <td>2024-10-05T22:00:33Z</td>\n",
       "      <td>gtTM9GJ8nZU</td>\n",
       "      <td>none</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>True</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>EL PELAO FILOSOF SOBRE EL MATE</td>\n",
       "      <td>2024-10-05T20:00:08Z</td>\n",
       "      <td>OppCmaXtZME</td>\n",
       "      <td>none</td>\n",
       "      <td>197</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>True</td>\n",
       "      <td>0.274112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>HOMI ELEVANDO EL NIVEL DEL PROGRAMA</td>\n",
       "      <td>2024-10-05T18:00:11Z</td>\n",
       "      <td>Q3LCtQoVFfE</td>\n",
       "      <td>none</td>\n",
       "      <td>3214</td>\n",
       "      <td>327</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>True</td>\n",
       "      <td>0.101742</td>\n",
       "      <td>0.000622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>EL MITO DEL PETISO PIJUDO EN HABLEMOS SIN SABER</td>\n",
       "      <td>2024-10-05T16:00:16Z</td>\n",
       "      <td>J8KDqUj9B2g</td>\n",
       "      <td>none</td>\n",
       "      <td>7745</td>\n",
       "      <td>552</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>True</td>\n",
       "      <td>0.071272</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OLGA</td>\n",
       "      <td>UC7mJ2EDXFomeDIRFu5FtEbA</td>\n",
       "      <td>NATI CORRI DROGADA POR MSTERDAM</td>\n",
       "      <td>2024-10-05T12:00:28Z</td>\n",
       "      <td>FJb5YHpWoDI</td>\n",
       "      <td>none</td>\n",
       "      <td>8722</td>\n",
       "      <td>507</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "      <td>0.058129</td>\n",
       "      <td>0.000917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel_name                channel_id  \\\n",
       "0         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "1         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "2         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "3         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "4         OLGA  UC7mJ2EDXFomeDIRFu5FtEbA   \n",
       "\n",
       "                                             title          published_at  \\\n",
       "0             BETU ES EL NUEVO REY DE LOS MACARONS  2024-10-05T22:00:33Z   \n",
       "1                  EL PELAO FILOSOF SOBRE EL MATE  2024-10-05T20:00:08Z   \n",
       "2              HOMI ELEVANDO EL NIVEL DEL PROGRAMA  2024-10-05T18:00:11Z   \n",
       "3  EL MITO DEL PETISO PIJUDO EN HABLEMOS SIN SABER  2024-10-05T16:00:16Z   \n",
       "4                NATI CORRI DROGADA POR MSTERDAM  2024-10-05T12:00:28Z   \n",
       "\n",
       "      video_id video_type  view_count  like_count  comment_count  \\\n",
       "0  gtTM9GJ8nZU       none         114          39              0   \n",
       "1  OppCmaXtZME       none         197          54              0   \n",
       "2  Q3LCtQoVFfE       none        3214         327              2   \n",
       "3  J8KDqUj9B2g       none        7745         552              3   \n",
       "4  FJb5YHpWoDI       none        8722         507              8   \n",
       "\n",
       "   duration_seconds  is_short  likes_per_view  comments_per_view  \n",
       "0                58      True        0.342105           0.000000  \n",
       "1                54      True        0.274112           0.000000  \n",
       "2                52      True        0.101742           0.000622  \n",
       "3                58      True        0.071272           0.000387  \n",
       "4                47      True        0.058129           0.000917  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itba-cde-pda-jT53MlBj-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
